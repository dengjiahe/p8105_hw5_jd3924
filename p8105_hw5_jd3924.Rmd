---
title: "p8105_hw5_jd3924"
author: "Jiahe Deng"
date: "2022-11-07"
output: github_document
---

```{r}
library(tidyverse)
library(p8105.datasets)
library(viridis)
knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```
## Problem 1

The code chunk below imports the data in individual spreadsheets contained in `./data/zip_data/`. To do this, I create a dataframe that includes the list of all files in that directory and the complete path to each file. As a next step, I `map` over paths and import data using the `read_csv` function. Finally, I `unnest` the result of `map`.

```{r}
full_df = 
  tibble(
    files = list.files("data/zip_data/"),
    path = str_c("data/zip_data/", files)
  ) %>% 
  mutate(data = map(path, read_csv)) %>% 
  unnest()
```

The result of the previous code chunk isn't tidy -- data are wide rather than long, and some important variables are included as parts of others. The code chunk below tides the data using string manipulations on the file, converting from wide to long, and selecting relevant variables. 

```{r}
tidy_df = 
  full_df %>% 
  mutate(
    files = str_replace(files, ".csv", ""),
    group = str_sub(files, 1, 3)) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "outcome",
    names_prefix = "week_") %>% 
  mutate(week = as.numeric(week)) %>% 
  select(group, subj = files, week, outcome)
```

Finally, the code chunk below creates a plot showing individual data, faceted by group. 

```{r}
tidy_df %>% 
  ggplot(aes(x = week, y = outcome, group = subj, color = group)) + 
  geom_point() + 
  geom_path() + 
  facet_grid(~group)
```

This plot suggests high within-subject correlation -- subjects who start above average end up above average, and those that start below average end up below average. Subjects in the control group generally don't change over time, but those in the experiment group increase their outcome in a roughly linear way. 

## Problem 2
```{r}
urlfile = "https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"
homicides_data = read_csv(url(urlfile), na = c(" ", "Unknown"))
```
In this data, there is 52179 observations and 12 variables.
This data include each case's detailed information, such as victim's name(victim_first,victim_race), age(victim_age),gender(victim_sex), race(victim_race),location(city, state, lat, lon), the progress of the case(disposition), case's reported data(reported_date).
```{r}
homicides_data = 
  homicides_data %>%
  mutate(
    city_state = str_c(city,state, sep = ","),
    case_status = ifelse(disposition %in% c("Closed without arrest","Open/No arrest"), "unsolved","solved")
         ) %>% relocate(city_state) %>% 
  filter(city_state != "Tulsa,AL")
homicides_data
```
```{r}
homicides_data %>%
  group_by(city_state) %>%
  summarize(
    number_of_unsolved = sum(case_status=="unsolved"),
    number_of_homicides = n())
```
```{r}
baltimore_data = 
  homicides_data %>%
  filter(city_state == "Baltimore,MD")

baltimore_summary = 
  baltimore_data %>%
  summarise(
    unsolved_md = sum(case_status == "unsolved"),
    num = n())

prop.test(
  x = baltimore_summary %>% pull(unsolved_md),
  n = baltimore_summary %>% pull(num)
  ) %>%
  broom::tidy()
```
```{r}
prop_test = function(city){
  
  city_summary = 
    city %>% 
      summarise(
        unsolved = sum(case_status == 'unsolved'),
        n = n()
      )
  
  city_test = 
    prop.test(
      x = city_summary %>% pull(unsolved),
      n = city_summary %>% pull(n)
    )
  
  city_test
}
```
```{r} 
#use Baltimore to test whether the function works
prop_test(baltimore_data)
```

```{r}
statistic_df =
  homicides_data %>%
  nest(-city_state) %>%
  mutate(
    result = map(data, prop_test),
    tidy_data = map(result, broom::tidy)
  ) %>%
  select(city_state, tidy_data) %>%
  unnest(tidy_data) %>%
  select(city_state, estimate, starts_with("conf"))
statistic_df
```
```{r}
statistic_df %>%
  mutate(city_state = fct_reorder(city_state, estimate,.desc = TRUE)) %>%
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  labs(
    title = "proportion of unsolved homicides",
    x = "location: city,state",
    y = "estimated proportion of unsolved homicides",
    caption = "Data from the Washington Post")
```

## Problem 3
```{r}
#Conduct a hypothesis T test for n = 30, sigma = 5
sim_mean_sd = function(n, mu, sigma) {
     sample_x = rnorm(n, mean = mu, sd = sigma)
     t_test = t.test(sample_x, conf.int = 0.95) %>%
       broom::tidy() %>%
       select(estimate, p.value)
     
     return(t_test)
}
output = vector("list", 5000)
for (i in 1:5000) {
  output[[i]] = sim_mean_sd(n = 30, mu = 0, sigma = 5)
}
output %>% bind_rows()
```

```{r}
#when mu={1,2,3,4,5,6}
results_df =
  tibble(mu = c(0, 1, 2, 3, 4, 5, 6)) %>%
  mutate(
    results = map(.x = mu, ~rerun(5000, sim_mean_sd(n=30,mu=.x,sigma=5))),
    estimated = map(results, bind_rows)) %>%
  unnest(estimated) %>%
  select(mu, estimate, p.value)
results_df
```


```{r}
power_test =
  results_df %>%
  mutate(
    reject = ifelse(p.value < 0.05, 1, 0)
      ) %>%
  group_by(mu) %>%
  summarize(
    n_reject = sum(reject),
    prop_reject = n_reject / n()
    ) %>%
  rbind()
power_test

power_test_plot =
  power_test %>%
  ggplot(aes(x = mu, y = prop_reject)) +geom_point(aes(color = mu), alpha = 0.5) +
  geom_smooth(alpha = 0.5) +
  theme(legend.position = "bottom") +
  labs(
    title = "plot of proportion of times the null was rejected vs true value of mu",
    x = "mu",
    y = "proportion of times the null was rejected") 
power_test_plot
```
From the plot above, we can see that there is positive relationship between the plot of proportion of times the null was rejected and the true mean, which means the proportion increase as the true mean increase, and the power of test increase too.
```{r}
library(patchwork)
average_estimate =
  results_df %>%
  group_by(mu) %>%
  summarize(
    avg = mean(estimate)
  )
average_estimate

avg_estimate_plot =
  average_estimate %>%
  ggplot(aes(x = mu, y = avg)) +
  geom_point(aes(color = mu), alpha = 0.5) +
  geom_smooth(alpha = 0.5) +
  theme(legend.position = "bottom") +
  labs(
    title = "avg estimate mu vs true mu",
    x = "mu",
    y = "average estimate mu"
) 

rej_average_estimate =
  results_df %>%
  filter(p.value < 0.05) %>%
  group_by(mu) %>%
  summarize(
    avg = mean(estimate)
  )
rej_average_estimate

rej_avg_estimate_plot =
  rej_average_estimate %>%
  ggplot(aes(x = mu, y = avg)) +
  geom_point(aes(color = mu), alpha = 0.5) +
  geom_smooth(alpha = 0.5) +
  theme(legend.position = "bottom") +
  labs(
    title = "avg estimate of rejected mu vs true mu",
    x = "mu",
    y = "average estimate of rejected mu"
)
rej_avg_estimate_plot+avg_estimate_plot
```
From the plot on the left, we can see that the average estimated mu in samples for which the null was rejected is slightly differ from the true mean, which display as not a small curve at the begging. But for mu>=2, there is no much differents between them. From the plot on the right, we can see that the average estimated mu is equal to the true mean. It is because if the average estimated mu is is equal to the true mean, then it could not be rejected, so there is a small curve on the beginning of the plot on the left. Also because our sample size is big enough, the test result is realiable.
